\documentclass{article}
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{booktabs}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{gensymb}
\usepackage{pdfpages}
\usepackage{mparhack}


\usepackage[bitstream-charter]{mathdesign}
\let\circledS\undefined
%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=2cm,left=1cm,right=7cm,marginparwidth=5cm, marginparsep=1cm]{geometry}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{principle}{Principle}[section]

%% Useful packages
\usepackage{amsmath}
\usepackage{bm}
\usepackage[makeroom]{cancel}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[final]{matlab-prettifier}
\usepackage{courier}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[]{appendix}
\usepackage{wrapfig}
\usepackage[final]{matlab-prettifier}
\usepackage[final]{mathtools}

\renewcommand\labelitemi{$\bullet$}

\definecolor{codecol}{rgb}{0.94, 0.94, 0.94}
\definecolor{dgreen}{rgb}{0.1, 0.49, 0.1}

\newcommand{\ea}{\nonumber \\}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=50pt,frame=bottomline}

\newcommand{\norm}[2]{\left\Vert#1\right\Vert_{#2}}
\newcommand{\question}[1]{{\color{red}\textbf{Question: }\emph{#1}}}
\newcommand{\action}[1]{{\color{blue}\textbf{Action: }\emph{#1}}}
\newcommand{\epdp}{$(\epsilon, \delta)$-DP}
\newcommand{\alg}{$\mathcal{A}$}
\newcommand{\data}{$\mathcal{D}$}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\normaldist}[2]{$\mathcal{N}(#1, #2)$}
\newcommand{\expt}[2]{\mathbb{E}_{#2}[#1]}
\newcommand{\prob}[2]{\mathbb{P}_{#2}[#1]}
\newcommand{\model}[1]{\mathcal{M}_{#1}}
\newcommand{\indep}{\raisebox{0.05em}{\rotatebox[origin=c]{90}{$\models$}}}
\newcommand{\mset}[3]{\lbrace {#1}_{#2} \rbrace_{#2=1}^{#3} }
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\ix}[1]{%
  \leavevmode % if at the start of a paragraph
  \marginpar{\small\emph{#1}}% the note
}

\newcommand{\qx}[1]{%
	\leavevmode % if at the start of a paragraph
	\marginpar{\color{blue}\small\emph{#1}}% the note
}
\newcommand{\marfig}[2]{
  \marginpar{ \includegraphics[width=\marginparwidth]{#1} \centering \text{\small #2} }
}

\newcommand{\comment}[1]{
	\textcolor{red}{\textbf{#1}}
}

\DeclareMathAlphabet{\altmathcal}{OMS}{cmsy}{m}{n}

\title{\textbf{Reinforcement Learning: An Introduction}\\
\textit{Summary Notes}
}
\author{Mrinank Sharma}

\begin{document}
\maketitle

\section{Introduction}
Other\ix{Elements of Reinforcement Learning} than the \textbf{agent} and the \textbf{environment}, there are four main subelements of an RL system:
\begin{enumerate}
	\item The \emph{policy} maps perceived states of the environment to actions. This could be a lookup table or a search process. 
	\item The \emph{reward signal} defines the goal of a reinforement learning problem. The environment sends rewards to an agent that has the sole objective of maximising this reward. 
	\item The \emph{value function} specifies what is good in the long run. The \emph{value} of a state is the total amount of reward an agent can expect to accumulate over teh future starting from that state. 
	\item Some RL systems have \emph{models} of the environment that can mimic the behaviour of the environment. Models are used for planning. 
\end{enumerate}
Whilst\ix{Rewards and Value Functions} rewards determine the immediate, intrisic desirability of environmental states, values indicate  the long-term desirability of states after taking into account which states are likely to follow. We choose actions based on values, even though values are derived from rewards.  

It\ix{Evolutionary Methods} is possible to learn policies without estimating value functions by using evolutionary methods. However, they tend to ignore much of the useful structure and are not well suited for RL tasks as they ignore relevant information, such as the states which are passed and the actions which are selected. 

\section{Multi-armed Bandits}
\begin{quote}
	\emph{The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that \textbf{evaluates} the actions taken rather than \textbf{instructs} by giving correct actions.}
\end{quote}
This form of feedback creates the need for active exploration. 

\begin{definition}[$k$-armed Bandit]
	\ix{$k$-armed bandits}In the $k$-armed bandit problem, you are repeatedly faced with a choice among $k$ different actions. After each choice, a numerical reward is received from a \textbf{stationary} probability distribution associated with the selected action. The objective is to maximise the expected total reward over some time period. 
\end{definition}
The \ix{Value}value of action $a$ is the expected reward given $a$ was chosen:
\begin{align}
q_*(a) = \mathbb{E}[R_t |\ A_t = a].
\end{align}
If we knew the values, problem solved - just pick the action with the maximum value. However, we do not know these. Denote the \emph{estimated} value as $Q_t(a)$. A simple way to estimate the action-value is a \emph{sample average}, which can be calculated incrementally. 

Note\ix{Reward Variance} that purely greedy methods are best if there is no noise in the reward signal. With $\epsilon$-greedy algorithms, the best value of $\epsilon$ will depend on the amount of noise, which determines the number of samples for the sample average to converge. However, if the problem is \emph{non-stationary}, we still need to explore.  

If\ix{Non-stationary Problems} the bandit is non-stationary, we might use the following rule for value estimates:
\begin{align}
Q_{n+1}(a) = Q_{n}(a) + \alpha_n(a)\ \big[ R_n - Q_n(a)\big].
\end{align}
It\ix{Convergence Guarantees} is known that convergence with probability $1$ happens if the following conditions hold:
\begin{align}
	\sum_{n=1}^{\infty} \alpha_n(a) = \infty \text{,  and } \sum_{n=1}^{\infty} \alpha_n(a)^2 < \infty.
\end{align}
The first condition ensures the steps are sufficiently large to overcome an initial condition and the second condition ensures that the steps become small enough to give convergence. However, step sizes which meet this conditions are rarely used in applications because they seem to converge slowly or require considerable tuning. 

With this techniques, the initialisation effectively becomes an additional parameter which needs to be chosen. They can be used to provide prior knowledge and can encourage exploration, for example, by being optimistic. However, this sort of exploration is not useful for non-stationary problems (the task changes creates a renewed need for exploration). 

The\ix{UCB} intuition behind UCB is that its better, when exploring, to select among the non-greedy actions which have the \emph{potential} for being optimal. To do this, we maximise an estimate of the arm's value summed with some sort of uncertainty estimate. UCB often performs well but can be difficult to extent to RL from bandits. 

We don't have to use value based methods. Instead, we could directly learn a preference over the actions and modify our preferences to give us higher rewards e.g., increasing our preference for an action if selecting it lead to a better outcome than expected. 

\begin{definition}[Contextual Bandits]
	In\ix{Contextual Bandits} a contextual bandit, the agent is faced with a series of multi-armed bandit problems, each associated with some information i.e., the agent has information about which bandit problem is being faced at any timestep. 
\end{definition}
Contextual bandits are intermediate between multi-armed bandits and the full RL problem; they involve learning a policy (i.e., a mapping from state information to actions) but similar to $k$-armed bandits, \textbf{the action selected influences only the immediate reward and not the next situation}. If we drop this constraint, we have the full RL problem. 

\section{Finite Markov Decision Processes}
\begin{definition}[Finite Markov Decision Process (MDP)]
	A\ix{MDP} finite Markov Decision Process (MDP) is made up of:
	\begin{itemize}
		\item A sequence of discrete time steps, $t = 0, 1, 2, \ldots$.
		\item A discrete set of states, $S_t \in \altmathcal{S}$.
		\item A set of actions which the agent can take, $A_t \in \altmathcal{A}(S_t)$.
		\item A finite set of numerical rewards, $R_t \in \altmathcal{R} \subset \mathbb{R}$.
	In this case, there is a well defined discrete transition distribution:
	\begin{align}
	p(s', r | s, a) \triangleq \text{Pr}[S_t = s', R_t =r| S_{t-1}=s, A_{t-1}=a],
	\end{align}
	which defines the \textbf{dynamics} of the MDP. 
	\end{itemize}
\end{definition}
Note the Markov assumption; probabilities over the next state and reward depend only on the current state and action.\ix{Interpreting the Markov Assumption} This can be interpreted as a restriction on the state - the state must include information about all aspects of the past agent-environment interaction which will affect the future. 

Given $p$, we can compute the following functions of interest:
\begin{align}
p(s'|s, a) \triangleq \text{Pr}[S_t = s' | S_{t-1}=s, A_{t-1}=a],
\end{align}
\begin{align}
r(s, a) \triangleq \mathbb{E}[R_t  | S_{t-1}=s, A_{t-1}=a],
\end{align}
and
\begin{align}
r(s, a, s') \triangleq \mathbb{E}[R_t  | S_{t-1}=s, A_{t-1}=a, S_t = s'].
\end{align}

A\ix{Agent vs Environment} general rule of thumb is that anything that cannot be changed \emph{arbitarily} by the agent is considered to be outside of it and thus part of the environment. The agent-environment boundary represents the limit of the agent's absolute control, not of its knowledge. 

The goal of the agent is formalised in terms of the reward signal which passes from the environment to the agent. This is one of the most distinctive features of RL. 
\begin{quote}
	\ix{Reward Hypothesis}\emph{All of what we mean by goals and purposes can be well though of as the maximisation of the expected value of the cumulative sum of a received scalar signal.}
\end{quote}
The reward signal is not the place to impart to the agent prior knowlege about \emph{how} to achieve a task; if we do this, the agent may only achieve subgoals without actually acheiving the real goal. A better place to impart this knowledge is the initial policy or value function. \comment{How does this link to reward shaping?}

\ix{Discount Rates}Episodic tasks terminate at some timestep, $T$, whilst continuing tasks do not. In the first case, we simply use the (expected) sum of rewards whilst we introduce discounting in the infinite horizon case i.e., 
\begin{align}
G_t \triangleq \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}.
\end{align}
$G_t$ is the \emph{discounted return}. The closer the value of $\gamma$ is to $1$, the more farsighted the agent is. 

\ix{Unifying Notation}We convert episodic tasks to continuing tasks by introducting a special \emph{absorbing state} which transitions only to itself and returning zero reward. We can write:
\begin{align}
G_t \triangleq \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k,
\end{align}
allowing for $T=\infty$ and $\gamma=1$. \comment{The book says not both - but I think that should be allowed for an episodic task with absorbing state.}

\begin{definition}[Policy]
	\ix{Policy}A policy is a mapping from states to probabilities of selecting each possible action. $\pi(a|s)$ is the distribution over actions, $A_t$, given the current state, $S_t$. 
\end{definition}

\begin{definition}[Value Function]
	The\ix{Value Function} value function of state $s$ under policy $\pi$ is the expected return when starting in $s$ and following $\pi$.
	\begin{align}
	v_\pi(s) \triangleq \mathbb{E}_\pi [G_t | S_t = s]
	\end{align}
\end{definition}

\begin{definition}[Action-Value Function]
	The\ix{Action-Value Function} action-value function of action $a$, state $s$ under policy $\pi$ is the expected return when starting in $s$ and following $\pi$ after first performing $a$.
	\begin{align}
	q_\pi(s, a) \triangleq \mathbb{E}_\pi [G_t | S_t = s, A_t = a]
	\end{align}
\end{definition}
Value functions satisfy recursive relationships. 

\end{document}